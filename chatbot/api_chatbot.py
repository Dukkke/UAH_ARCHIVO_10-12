"""
API Chatbot del Archivo Patrimonial UAH - Versi√≥n con Patrones de Dise√±o
=========================================================================

PATRONES DE DISE√ëO IMPLEMENTADOS:
=================================

1. ABSTRACT FACTORY (Creacional) - services/factory.py
   ‚îú‚îÄ‚îÄ AbstractServiceFactory (ABC)
   ‚îÇ   ‚îú‚îÄ‚îÄ GeminiServiceFactory  ‚Üí Servicios con API de Gemini
   ‚îÇ   ‚îú‚îÄ‚îÄ LocalServiceFactory   ‚Üí Servicios locales sin API
   ‚îÇ   ‚îî‚îÄ‚îÄ MockServiceFactory    ‚Üí Servicios mock para testing
   ‚îî‚îÄ‚îÄ M√©todos: create_embedder(), create_responder(), create_llm_proxy()

2. PROXY (Estructural) - services/llm_proxy.py
   ‚îú‚îÄ‚îÄ LLMProxy (ABC)
   ‚îÇ   ‚îú‚îÄ‚îÄ GeminiClientProxy     ‚Üí Protection Proxy (circuit breaker)
   ‚îÇ   ‚îú‚îÄ‚îÄ CachingLLMProxy       ‚Üí Caching Proxy (TTL 1 hora)
   ‚îÇ   ‚îú‚îÄ‚îÄ LoggingLLMProxy       ‚Üí Logging Proxy (estad√≠sticas)
   ‚îÇ   ‚îú‚îÄ‚îÄ NullLLMProxy          ‚Üí Null Object (modo offline)
   ‚îÇ   ‚îî‚îÄ‚îÄ MockLLMProxy          ‚Üí Mock para testing
   ‚îî‚îÄ‚îÄ M√©todos: embed(), generate(), is_available()

3. STRATEGY (Comportamiento) - services/search_strategies.py
   ‚îú‚îÄ‚îÄ SearchStrategy (ABC)
   ‚îÇ   ‚îú‚îÄ‚îÄ ExactTitleSearchStrategy   ‚Üí Match exacto en t√≠tulos
   ‚îÇ   ‚îú‚îÄ‚îÄ TFIDFSearchStrategy        ‚Üí B√∫squeda TF-IDF local
   ‚îÇ   ‚îú‚îÄ‚îÄ SemanticSearchStrategy     ‚Üí Embeddings sem√°nticos
   ‚îÇ   ‚îú‚îÄ‚îÄ MetadataSearchStrategy     ‚Üí Dublin Core ponderado
   ‚îÇ   ‚îî‚îÄ‚îÄ HybridSearchStrategy       ‚Üí RRF fusion de estrategias
   ‚îî‚îÄ‚îÄ Context: SearchContext(strategy).search(query, documents)

4. OBSERVER (Comportamiento) - services/events.py
   ‚îî‚îÄ‚îÄ EventBus + LoggingObserver para logging de eventos

PRINCIPIOS SOLID APLICADOS:
===========================

[S] Single Responsibility (SRP):
    - Cada Strategy tiene una √∫nica responsabilidad (un algoritmo)
    - Cada Proxy tiene una √∫nica responsabilidad (una capa)
    - Cada Factory crea una familia espec√≠fica de objetos

[O] Open/Closed (OCP):
    - Nuevas estrategias de b√∫squeda sin modificar SearchContext
    - Nuevas factories sin modificar c√≥digo cliente
    - Nuevos proxies decorando los existentes

[L] Liskov Substitution (LSP):
    - Todas las SearchStrategy son intercambiables
    - Todos los LLMProxy son intercambiables
    - Todas las AbstractServiceFactory son intercambiables

[I] Interface Segregation (ISP):
    - LLMProxy: solo embed(), generate(), is_available()
    - SearchStrategy: solo search(), get_name()
    - Interfaces m√≠nimas y espec√≠ficas

[D] Dependency Inversion (DIP):
    - api_chatbot depende de AbstractServiceFactory, no de GeminiServiceFactory
    - SearchContext depende de SearchStrategy, no de implementaciones concretas
    - Inyecci√≥n de dependencias en constructores

Funcionalidades:
- Detecci√≥n de conversaciones casuales vs b√∫squedas
- Respuestas sin b√∫squeda para saludos/despedidas
- B√∫squeda h√≠brida (exacta + TF-IDF + sem√°ntica + metadata)
- Enlaces a documentos espec√≠ficos
- Manejo de errores robusto
- Puerto 5000 (Flask) + Frontend en 8080 (Nginx)
"""

import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Callable
import random
import pickle
import json
import os
import traceback
import pytz

# Flask imports
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import markdown

# Google Generative AI
import google.generativeai as genai
from dotenv import load_dotenv

# ============================================================================
# IMPORTACI√ìN DE PATRONES DE DISE√ëO
# ============================================================================

# ABSTRACT FACTORY Pattern (Creacional)
# Proporciona interfaz para crear familias de objetos relacionados
from services.factory import (
    AbstractServiceFactory,      # Interfaz abstracta (DIP)
    GeminiServiceFactory,        # Factory concreta para Gemini
    LocalServiceFactory,         # Factory concreta para modo offline
    ServiceFactory,              # Alias backward compatible
    create_service_factory       # Factory Method
)

# PROXY Pattern (Estructural)
# Controla acceso al servicio LLM con caching, logging, protecci√≥n
from services.llm_proxy import (
    LLMProxy,                    # Interfaz abstracta (DIP)
    GeminiClientProxy,           # Protection Proxy
    CachingLLMProxy,             # Caching Proxy
    LoggingLLMProxy,             # Logging Proxy
    NullLLMProxy,                # Null Object Pattern
    create_production_proxy      # Factory Method para proxies
)

# STRATEGY Pattern (Comportamiento)
# Algoritmos de b√∫squeda intercambiables
from services.search_strategies import (
    SearchStrategy,              # Interfaz abstracta (DIP)
    ExactTitleSearchStrategy,    # Estrategia: match exacto
    TFIDFSearchStrategy,         # Estrategia: TF-IDF
    SemanticSearchStrategy,      # Estrategia: embeddings
    MetadataSearchStrategy,      # Estrategia: Dublin Core
    HybridSearchStrategy,        # Estrategia: combinaci√≥n RRF
    SearchContext,               # Context del Strategy Pattern
    create_search_strategy       # Factory Method para estrategias
)

# OBSERVER Pattern (Comportamiento) + Strategy adicionales
from services.events import EventBus, LoggingObserver
from services.conversation import (
    ConversationSession, 
    IntentionDetector,           # Strategy: Detecci√≥n de intenci√≥n
    EntityExtractorImpl,         # Strategy: Extracci√≥n de entidades
    EntityExtractor,             # Alias backward compatible
    DocumentComparator,          # Strategy: Comparaci√≥n de documentos
    FuzzyEntityExtractor,        # Strategy: Fuzzy matching
    FuzzyDocumentComparator,     # Strategy: Fuzzy similarity
    SynonymExpander,             # Strategy: Expansi√≥n de sin√≥nimos
    DateRangeExtractor,          # Strategy: Extracci√≥n de fechas
    MetadataSearcher             # Composite: B√∫squeda ponderada
)

# Machine Learning
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# ============================================================================
# CONFIGURACI√ìN INICIAL
# ============================================================================

app = Flask(__name__)
CORS(app)

# Configuraci√≥n de Gemini
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyBnVgg33jVHSypAkDqv-6PFTtqK8-eh3dM")
GENAI_AVAILABLE = False

if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        GENAI_AVAILABLE = True
    except Exception as _e:
        GENAI_AVAILABLE = False
        print("‚ö†Ô∏è No se pudo inicializar Gemini. Continuando sin GENAI.")

embeddings_ready = False

# ============================================================================
# ABSTRACT FACTORY Pattern - Creaci√≥n de servicios
# ============================================================================
# Principio DIP: Dependemos de la abstracci√≥n AbstractServiceFactory,
# no de la implementaci√≥n concreta GeminiServiceFactory.
# Esto permite cambiar la factory sin modificar este c√≥digo.

factory: AbstractServiceFactory = GeminiServiceFactory(genai, GENAI_AVAILABLE)

# ALTERNATIVE: Usar LocalServiceFactory para modo offline
# factory: AbstractServiceFactory = LocalServiceFactory()

# ============================================================================
# OBSERVER Pattern - Sistema de eventos
# ============================================================================
# Principio OCP: Nuevos observers pueden agregarse sin modificar el EventBus.
# Principio SRP: EventBus solo maneja suscripciones, observers manejan acciones.

event_bus = EventBus()
event_bus.subscribe('chat.received', LoggingObserver('[chat.received] '))
event_bus.subscribe('chat.type_detected', LoggingObserver('[chat.type_detected] '))
event_bus.subscribe('search.done', LoggingObserver('[search.done] '))
event_bus.subscribe('response.generated', LoggingObserver('[response.generated] '))

# Almacenamiento de sesiones conversacionales (en memoria para MVP)
conversation_sessions = {}

# ============================================================================
# STRATEGY Pattern - Estrategias de conversaci√≥n (DIP)
# ============================================================================
# Principio DIP: Inyecci√≥n de dependencias - instancias intercambiables.
# Principio OCP: Nuevas estrategias sin modificar c√≥digo existente.
# Principio LSP: Todas las estrategias son intercambiables.

# Strategy: Detecci√≥n de intenci√≥n del usuario
intention_detector = IntentionDetector()

# Strategy: Extracci√≥n de entidades con fuzzy matching (tolera typos)
entity_extractor = FuzzyEntityExtractor()

# Strategy: Comparaci√≥n de documentos
document_comparator = DocumentComparator()

# Strategy: Expansi√≥n de sin√≥nimos para b√∫squedas
synonym_expander = SynonymExpander()

# Strategy: Extracci√≥n de rangos de fechas hist√≥ricas
date_extractor = DateRangeExtractor()

# ============================================================================
# STRATEGY Pattern - Estrategia de b√∫squeda h√≠brida
# ============================================================================
# Principio OCP: Nuevas estrategias de b√∫squeda sin modificar SearchContext.
# Principio DIP: SearchContext depende de SearchStrategy abstracci√≥n.

# Crear estrategia h√≠brida que combina m√∫ltiples algoritmos
search_strategy: SearchStrategy = HybridSearchStrategy()

# Agregar estrategias con sus pesos (principio Composite)
search_strategy.add_strategy(ExactTitleSearchStrategy(), weight=1.5)
search_strategy.add_strategy(MetadataSearchStrategy(), weight=1.2)
search_strategy.add_strategy(TFIDFSearchStrategy(), weight=1.0)

# Context del Strategy Pattern
search_context = SearchContext(search_strategy)

# ============================================================================
# ============================================================================

def load_documents():
    """Carga los documentos desde clean_with_metadata.json"""
    try:
        with open('clean_with_metadata.json', 'r', encoding='utf-8', errors="ignore") as f:
            docs = json.load(f)
        print(f"‚úÖ Documentos cargados: {len(docs)}")
        return docs
    except FileNotFoundError:
        print("‚ùå Archivo clean_with_metadata.json no encontrado")
        return []
    except Exception as e:
        print(f"‚ùå Error cargando documentos: {e}")
        return []

documents = load_documents()

# ============================================================================
# √çNDICE TF-IDF LOCAL (sin necesidad de API)
# ============================================================================

def load_tfidf_index():
    """Carga el √≠ndice TF-IDF local para b√∫squeda sin API"""
    try:
        with open('search_index.pkl', 'rb') as f:
            index_data = pickle.load(f)
        print(f"‚úÖ √çndice TF-IDF cargado: {index_data['matrix'].shape[0]} docs x {index_data['matrix'].shape[1]} t√©rminos")
        return index_data
    except FileNotFoundError:
        print("‚ö†Ô∏è search_index.pkl no encontrado. Ejecuta create_search_index.py primero.")
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è Error cargando √≠ndice TF-IDF: {e}")
        return None

tfidf_index = load_tfidf_index()

def search_exact_title(query, top_k=15):
    """
    B√∫squeda EXACTA por t√≠tulo - prioriza matches exactos y parciales.
    Se ejecuta ANTES de TF-IDF para encontrar documentos con t√≠tulo exacto.
    """
    query_lower = query.lower().strip()
    results = []
    
    for idx, doc in enumerate(documents):
        title = doc.get('title', '').lower()
        href = doc.get('href', '').lower()
        
        score = 0
        
        # Match exacto de t√≠tulo (m√°xima prioridad)
        if query_lower == title:
            score = 1.0
        # T√≠tulo contiene la query completa
        elif query_lower in title:
            score = 0.9
        # Query contiene el t√≠tulo completo
        elif title and title in query_lower:
            score = 0.85
        # Match en URL/href
        elif query_lower.replace(' ', '-') in href or query_lower.replace(' ', '') in href:
            score = 0.8
        # Match parcial de palabras clave
        else:
            query_words = set(query_lower.split())
            title_words = set(title.split())
            common = query_words & title_words
            if len(common) >= 3:  # Al menos 3 palabras en com√∫n
                score = 0.6 + (len(common) / len(query_words)) * 0.2
        
        if score > 0.5:
            doc_copy = doc.copy()
            doc_copy['relevance_score'] = score
            doc_copy['_match_type'] = 'exact_title'
            results.append((score, idx, doc_copy))
    
    # Ordenar por score descendente
    results.sort(key=lambda x: x[0], reverse=True)
    
    return [r[2] for r in results[:top_k]]

def search_with_tfidf(query, top_k=15):
    """B√∫squeda usando TF-IDF local (r√°pida, sin API)"""
    if not tfidf_index:
        return []
    
    try:
        vectorizer = tfidf_index['vectorizer']
        tfidf_matrix = tfidf_index['matrix']
        
        # Vectorizar la consulta
        query_vector = vectorizer.transform([query])
        
        # Calcular similitudes usando coseno
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
        
        # Obtener √≠ndices de los top_k m√°s similares
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if idx < len(documents) and similarities[idx] > 0.01:  # Umbral m√≠nimo
                doc = documents[idx].copy()
                doc['relevance_score'] = float(similarities[idx])
                results.append(doc)
        
        return results
    except Exception as e:
        print(f"‚ùå Error en b√∫squeda TF-IDF: {e}")
        return []

def load_embeddings():
    """
    Carga embeddings desde pickle.
    
    Soporta dos formatos:
    1. Dict con claves 'embeddings', 'texts', 'documents' (embeddings_oficial.pkl)
    2. Dict simple {idx: embedding} (embeddings_cache.pkl antiguo)
    """
    embeddings_files = ['embeddings_compatible.pkl', 'embeddings_oficial.pkl', 'embeddings_cache.pkl']
    
    for filename in embeddings_files:
        try:
            with open(filename, 'rb') as f:
                data = pickle.load(f)
            
            # Formato 1: Dict estructurado con claves 'embeddings', 'texts', 'documents'
            if isinstance(data, dict) and 'embeddings' in data:
                embeddings_array = data['embeddings']
                num_embeddings = len(embeddings_array) if hasattr(embeddings_array, '__len__') else 0
                if num_embeddings > 0:
                    print(f"‚úÖ Embeddings cargados desde {filename}: {num_embeddings} documentos")
                    # Retornar el diccionario completo para acceso a embeddings, texts, documents
                    return data
            
            # Formato 2: Dict simple {idx: embedding}
            elif isinstance(data, dict) and len(data) > 0:
                # Verificar si las claves son √≠ndices num√©ricos
                first_key = next(iter(data.keys()), None)
                if isinstance(first_key, int) or (isinstance(first_key, str) and first_key.isdigit()):
                    print(f"‚úÖ Embeddings cargados desde {filename}: {len(data)} documentos")
                    return data
                    
        except FileNotFoundError:
            continue
        except Exception as e:
            print(f"‚ö†Ô∏è Error cargando {filename}: {e}")
            continue
    
    print("‚ö†Ô∏è No se encontraron embeddings precalculados. Usando b√∫squeda TF-IDF.")
    return {}

def create_embeddings_fallback():
    """Crea embeddings nuevos de manera optimizada (por lotes)"""
    embeddings = {}
    try:
        if not GENAI_AVAILABLE:
            print("‚ùå GENAI no disponible; no se pueden crear embeddings nuevos.")
            return {}
        print("üîÑ Generando embeddings nuevos (Optimizado por lotes)...")
        
        batch_size = 100
        texts_to_embed = []
        indices_map = []
        
        # Preparar textos
        for idx, doc in enumerate(documents):
            text = f"{doc['title']} {doc['href']}"
            texts_to_embed.append(text)
            indices_map.append(idx)
            
        total_docs = len(texts_to_embed)
        
        # Procesar en lotes
        for i in range(0, total_docs, batch_size):
            batch_texts = texts_to_embed[i:i + batch_size]
            batch_indices = indices_map[i:i + batch_size]
            
            try:
                # Llamada batch a la API
                result = genai.embed_content(
                    model="models/text-embedding-004",
                    content=batch_texts,
                    task_type="retrieval_document"
                )
                
                # Extraer embeddings del resultado
                if 'embedding' in result:
                    batch_embeddings = result['embedding']
                    for j, embed in enumerate(batch_embeddings):
                        original_idx = batch_indices[j]
                        embeddings[original_idx] = embed
                
                print(f"   Procesados {min(i + batch_size, total_docs)}/{total_docs} documentos")
                
            except Exception as e:
                print(f"‚ùå Error en lote {i}-{i+batch_size}: {e}")
                # Fallback: intentar uno por uno en este lote si falla el batch
                for j, text in enumerate(batch_texts):
                    try:
                        res = genai.embed_content(
                            model="models/text-embedding-004",
                            content=text,
                            task_type="retrieval_document"
                        )
                        embeddings[batch_indices[j]] = res['embedding']
                    except Exception as inner_e:
                        print(f"   ‚ùå Error en documento individual: {inner_e}")
                        continue
        
        # Guardar cache
        try:
            with open('embeddings_cache.pkl', 'wb') as f:
                pickle.dump(embeddings, f)
            print(f"üíæ Cache guardado: embeddings_cache.pkl")
        except Exception as e:
            print(f"‚ö†Ô∏è No se pudo guardar cache: {e}")
            
        print(f"‚úÖ Embeddings creados: {len(embeddings)} documentos")
        return embeddings
        
    except Exception as e:
        print(f"‚ùå Error cr√≠tico creando embeddings: {e}")
        return {}

document_embeddings = load_embeddings()

def get_embeddings_count(emb_data):
    """Helper para obtener el conteo correcto de embeddings."""
    if isinstance(emb_data, dict) and 'embeddings' in emb_data:
        return len(emb_data['embeddings'])
    return len(emb_data) if emb_data else 0

embeddings_ready = bool(document_embeddings)
embeddings_count = get_embeddings_count(document_embeddings)

# ============================================================================
# B√öSQUEDA SEM√ÅNTICA
# ============================================================================

def search_documents(query, top_k=15, include_suggestions=True):
    """
    Busca documentos usando similitud sem√°ntica
    Retorna hasta top_k documentos m√°s relevantes y sugerencias de refinamiento
    """
    try:
        # Normalizar query
        normalized_query = normalize_query(query)
        print(f"üîç Query normalizada: '{normalized_query}'")
        
        # PASO 1: Buscar matches EXACTOS por t√≠tulo primero
        exact_results = search_exact_title(query, top_k)
        if exact_results:
            print(f"‚úÖ B√∫squeda exacta encontr√≥ {len(exact_results)} documentos")
            suggestions = generate_search_suggestions(query, exact_results) if include_suggestions else []
            return exact_results, suggestions
        
        # PASO 2: Si no hay matches exactos, usar TF-IDF
        if tfidf_index:
            print("üîÑ Usando b√∫squeda TF-IDF local...")
            results = search_with_tfidf(normalized_query, top_k)
            if results:
                print(f"üìÑ TF-IDF encontr√≥ {len(results)} documentos")
                suggestions = generate_search_suggestions(query, results) if include_suggestions else []
                return results, suggestions
            else:
                print("‚ö†Ô∏è TF-IDF sin resultados; probando keywords...")
        
        # PASO 3: Fallback a b√∫squeda por keywords
        results = search_by_keywords(normalized_query, top_k)
        suggestions = generate_search_suggestions(query, results) if include_suggestions else []
        return results, suggestions
        
        # Generar embedding de la consulta (via factory/proxy)
        query_embedder = factory.make_query_embedding()
        query_embedding = None
        embed = query_embedder(normalized_query)
        if embed is None:
            # Fallback a b√∫squeda por keywords si no hay embedding
            print("‚ö†Ô∏è No se pudo generar embedding; usando b√∫squeda por keywords...")
            results = search_by_keywords(normalized_query, top_k)
            suggestions = generate_search_suggestions(query, results) if include_suggestions else []
            return results, suggestions
        else:
            query_embedding = embed

        # Calcular similitudes
        similarities = []
        for idx, doc_embedding in document_embeddings.items():
            similarity = cosine_similarity(
                [query_embedding],
                [doc_embedding]
            )[0][0]
            similarities.append((idx, similarity))

        # Ordenar por similitud
        similarities.sort(key=lambda x: x[1], reverse=True)

        # Retornar top_k documentos
        results = []
        for idx, score in similarities[:top_k]:
            if idx < len(documents):
                doc = documents[idx].copy()
                doc['relevance_score'] = float(score)
                results.append(doc)

        print(f"üìÑ Encontrados {len(results)} documentos relevantes")
        
        # Generar sugerencias si se solicita
        suggestions = []
        if include_suggestions:
            suggestions = generate_search_suggestions(query, results)
            if suggestions:
                print(f"üí° Generadas {len(suggestions)} sugerencias de refinamiento")
        
        event_bus.publish('search.done', {'query': normalized_query, 'results_count': len(results), 'suggestions_count': len(suggestions)})
        return results, suggestions
        
    except Exception as e:
        print(f"‚ùå Error en b√∫squeda: {e}")
        traceback.print_exc()
        return [], []

# ============================================================================
# NORMALIZACI√ìN DE QUERIES
# ============================================================================

def is_query_too_generic(query, min_word_length=15):
    """Detecta si una consulta es muy gen√©rica y necesita refinamiento"""
    # T√©rminos muy amplios que suelen dar muchos resultados poco espec√≠ficos
    generic_terms = [
        'dictadura', 'gobierno', 'pol√≠tica', 'documentos', 'archivos',
        'historia', 'chile', 'informaci√≥n', 'datos', 'material',
        'fotograf√≠as', 'fotos', 'im√°genes', 'derechos', 'humanos',
        'partido', 'movimiento', 'organizaci√≥n'
    ]
    
    query_lower = query.lower().strip()
    words = query_lower.split()
    
    # Si la consulta es muy corta (1-2 palabras) y coincide con t√©rminos gen√©ricos
    if len(words) <= 2 and len(query_lower) < min_word_length:
        if any(term in query_lower for term in generic_terms):
            return True
    
    return False

def extract_categories_from_results(results):
    """Analiza t√≠tulos de resultados para extraer categor√≠as/temas comunes"""
    if not results:
        return []
    
    import re
    from collections import Counter
    
    # Extraer palabras clave significativas de los t√≠tulos
    keywords = []
    for doc in results:
        title = doc.get('title', '')
        # Extraer a√±os
        years = re.findall(r'\b(19\d{2}|20\d{2})\b', title)
        keywords.extend(years)
        
        # Extraer palabras significativas (>3 caracteres, no stopwords)
        stopwords = {'de', 'la', 'el', 'los', 'las', 'del', 'para', 'por', 'con', 'en', 'a', 'y', 'o', 'un', 'una'}
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√±]{4,}\b', title.lower())
        keywords.extend([w for w in words if w not in stopwords])
    
    # Contar frecuencias
    counter = Counter(keywords)
    # Retornar las 5 m√°s comunes (excluyendo la consulta original)
    return [word for word, count in counter.most_common(8) if count > 1]

def generate_search_suggestions(query, results):
    """Genera sugerencias de refinamiento basadas en consulta y resultados"""
    suggestions = []
    categories = extract_categories_from_results(results)
    
    query_lower = query.lower()
    
    # Sugerencias basadas en categor√≠as encontradas
    if categories:
        # Filtrar categor√≠as que ya est√°n en la query
        new_categories = [cat for cat in categories[:5] if cat not in query_lower]
        if new_categories:
            suggestions.append({
                'type': 'refine_by_category',
                'message': 'üéØ **Refina tu b√∫squeda combinando con estos temas:**',
                'options': new_categories
            })
    
    # Detectar si hay a√±os en los resultados
    import re
    years_in_results = set()
    for doc in results:
        years = re.findall(r'\b(19\d{2}|20\d{2})\b', doc.get('title', ''))
        years_in_results.update(years)
    
    if years_in_results and not re.search(r'\b(19\d{2}|20\d{2})\b', query):
        sorted_years = sorted(years_in_results)[:5]
        suggestions.append({
            'type': 'add_year',
            'message': 'üìÖ **Prueba especificar un a√±o:**',
            'options': sorted_years
        })
    
    # Sugerencias de especificidad
    if is_query_too_generic(query):
        suggestions.append({
            'type': 'be_more_specific',
            'message': 'üí° **Tu b√∫squeda es amplia. Prueba siendo m√°s espec√≠fico:**',
            'options': [
                f'"{query} a√±os 70"',
                f'"{query} 1973"',
                f'"{query} documentos"',
                f'"{query} fotograf√≠as"'
            ]
        })
    
    return suggestions

def normalize_query(query):
    """Normaliza y expande consultas para mejor b√∫squeda"""
    import unicodedata
    
    if not isinstance(query, str):
        return ""
        
    normalized = query.lower().strip()
    
    # Remover acentos
    normalized = ''.join(
        c for c in unicodedata.normalize('NFD', normalized)
        if unicodedata.category(c) != 'Mn'
    )
    
    # Stemming b√°sico para plurales (igual que en create_search_index.py)
    words = normalized.split()
    stemmed_words = []
    for word in words:
        # Si termina en 'es' (√°rboles -> √°rbol, canciones -> cancion)
        if word.endswith('es') and len(word) > 4:
            word = word[:-2]
        # Si termina en 's' (casas -> casa)
        elif word.endswith('s') and len(word) > 3 and not word.endswith('ss'):
            word = word[:-1]
        stemmed_words.append(word)
    
    normalized = ' '.join(stemmed_words)
    
    # Mapeo de t√©rminos y abreviaturas comunes
    term_mapping = {
        'dicta': 'dictadura militar',
        'ddhh': 'derechos humanos',
        'dd.hh': 'derechos humanos',
        'dd hh': 'derechos humanos',
        'mir': 'movimiento izquierda revolucionaria',
        'pc': 'partido comunista',
        'ps': 'partido socialista',
        'pdc': 'partido democrata cristiano',
        'golpe': 'golpe estado 1973',
        'pinochet': 'dictadura militar pinochet',
        'allende': 'salvador allende',
        'aylwin': 'patricio aylwin',
        '73': '1973',
        '74': '1974',
        '75': '1975',
        '76': '1976',
        '80': '1980',
        '90': '1990',
        'fotos': 'fotografias',
        'imagenes': 'fotografias',
        'pics': 'fotografias',
    }
    
    # Aplicar mapeo
    for abbrev, full_term in term_mapping.items():
        if abbrev in normalized:
            normalized = normalized.replace(abbrev, full_term)
    
    return normalized

def search_by_keywords(query, top_k=6):
    """B√∫squeda fallback por palabras clave cuando GENAI no est√° disponible"""
    query_lower = query.lower()
    query_words = set(query_lower.split())
    
    scored_docs = []
    for idx, doc in enumerate(documents):
        title_lower = doc['title'].lower()
        # Calcular score basado en coincidencias de palabras
        title_words = set(title_lower.split())
        common_words = query_words.intersection(title_words)
        
        # Score: n√∫mero de palabras en com√∫n + bonus por coincidencia exacta
        score = len(common_words)
        if query_lower in title_lower:
            score += 10  # Bonus por substring exacto
        
        if score > 0:
            doc_copy = doc.copy()
            doc_copy['relevance_score'] = float(score)
            scored_docs.append((score, idx, doc_copy))
    
    # Ordenar por score descendente
    scored_docs.sort(key=lambda x: x[0], reverse=True)
    
    # Retornar top_k
    results = [doc for _, _, doc in scored_docs[:top_k]]
    print(f"üìÑ Encontrados {len(results)} documentos por keywords")
    return results
    
    # Remover acentos
    normalized = ''.join(
        c for c in unicodedata.normalize('NFD', normalized)
        if unicodedata.category(c) != 'Mn'
    )
    
    # Mapeo de t√©rminos y abreviaturas comunes
    term_mapping = {
        'dicta': 'dictadura militar',
        'ddhh': 'derechos humanos',
        'dd.hh': 'derechos humanos',
        'dd hh': 'derechos humanos',
        'mir': 'movimiento izquierda revolucionaria',
        'pc': 'partido comunista',
        'ps': 'partido socialista',
        'pdc': 'partido democrata cristiano',
        'golpe': 'golpe estado 1973',
        'pinochet': 'dictadura militar pinochet',
        'allende': 'salvador allende',
        'aylwin': 'patricio aylwin',
        '73': '1973',
        '74': '1974',
        '75': '1975',
        '76': '1976',
        '80': '1980',
        '90': '1990',
        'fotos': 'fotografias',
        'imagenes': 'fotografias',
        'pics': 'fotografias',
    }
    
    # Aplicar mapeo
    for abbrev, full_term in term_mapping.items():
        if abbrev in normalized:
            normalized = normalized.replace(abbrev, full_term)
    
    return normalized

# ============================================================================
# DETECCI√ìN DE TIPO DE CONVERSACI√ìN
# ============================================================================

def detect_conversation_type(query):
    """
    Detecta el tipo de conversaci√≥n del usuario
    Retorna: 'greeting', 'farewell', 'gratitude', 'help', 'smalltalk', 'search'
    """
    query_lower = query.lower().strip()
    
    # Eliminar puntuaci√≥n
    query_clean = re.sub(r'[¬ø?¬°!.,;:]', '', query_lower)
    
    # Patrones de saludos
    greetings = [
        r'^(hola|hello|hi|hey|ola)\s*(,|buenas?|d√≠as?|dias?|noches?|tardes?)?$',  # "hola", "hola buenas", "hola d√≠as"
        r'^(buenas|buenos)\s*(noches?|d√≠as?|dias?|tardes?)?$',  # "buenas", "buenas noches"
        r'^buen(os|as)?\s+(d√≠a|dia|d√≠as|dias|tarde|tardes|noche|noches)$',
        r'^(qu√©|que)\s+tal$',
        r'^c√≥mo\s+(est√°s|estas|est√°|esta)$',
        r'^saludos$',
        r'^\s*(hola\s+)?buen(os|as)?\s*$',  # "buenos" o "hola buenos"
    ]
    
    # Patrones de despedida
    farewells = [
        r'\b(adi√≥s|adios|chao|chau|bye|hasta\s+luego|nos\s+vemos)\b',
        r'\bgracias?\s+(por\s+todo|y\s+adi√≥s|y\s+adios)\b',
        r'^(chao|adios|adi√≥s|bye)$',
    ]
    
    # Patrones de agradecimiento
    gratitude = [
        r'^(gracias|muchas\s+gracias|mil\s+gracias)$',
        r'^(gracias|thank\s+you)$',
        r'^(excelente|genial|perfecto|muy\s+bien)$',
        r'^\bte\s+agradezco\b$',
    ]
    
    # Patrones de ayuda
    help_patterns = [
        r'\b(ayuda|ayudar|ay√∫dame|help)\b',
        r'^(qu√©|que)\s+(puedes?|hace|ofrece|tiene)$',
        r'^c√≥mo\s+(funciona|usar|buscar|te\s+uso)$',
        r'^(informaci√≥n|info|explica|cu√©ntame)\s+(sobre|del|de)?\s*(bot|chatbot|ti)?$',
        r'^qu√©\s+es\s+esto$',
        r'^para\s+qu√©\s+sirve',
    ]
    
    # Patrones de smalltalk
    smalltalk = [
        r'^(c√≥mo|como)\s+(te\s+llamas?|eres|funcionas)$',
        r'^(qui√©n|quien)\s+eres$',
        r'^(qu√©|que)\s+(eres|haces)$',
        r'^\best√°s\s+(bien|ah√≠)$',
        r'^eres\s+(un\s+)?(bot|robot|ia|inteligencia)$',
    ]
    
    # Verificar cada categor√≠a
    for pattern in greetings:
        if re.search(pattern, query_clean):
            return 'greeting'
    
    for pattern in farewells:
        if re.search(pattern, query_clean):
            return 'farewell'
    
    for pattern in gratitude:
        if re.search(pattern, query_clean):
            return 'gratitude'
    
    for pattern in help_patterns:
        if re.search(pattern, query_clean):
            return 'help'
    
    for pattern in smalltalk:
        if re.search(pattern, query_clean):
            return 'smalltalk'
    
    # Palabras casuales cortas
    words = query_clean.split()
    if len(words) <= 2 and len(query_clean) < 15:
        casual_words = ['ok', 'vale', 'ya', 'si', 's√≠', 'no', 'bien', 'mal', 'bueno']
        if any(word in casual_words for word in words):
            return 'smalltalk'
    
    # Por defecto, es una b√∫squeda
    return 'search'

# ============================================================================
# RESPUESTAS CONVERSACIONALES
# ============================================================================

def generate_conversational_response(query, conversation_type):
    """Genera respuestas naturales seg√∫n el tipo de conversaci√≥n"""
    
    # Obtener hora en zona horaria de Chile (Santiago)
    chile_tz = pytz.timezone('America/Santiago')
    hour = datetime.now(chile_tz).hour
    time_greeting = "Buenos d√≠as" if 5 <= hour < 12 else "Buenas tardes" if 12 <= hour < 20 else "Buenas noches"
    
    responses = {
        'greeting': [
            f"¬°{time_greeting}! üëã Soy el asistente del Archivo Patrimonial UAH.\n\n¬øQu√© documento hist√≥rico buscas hoy?",
            
            f"¬°Hola! üòä Archivo Patrimonial UAH a tu servicio.\n\nPuedo ayudarte con documentos, fotos y material hist√≥rico de Chile (1973-actualidad).\n\n¬øQu√© buscas?",
            
            f"{time_greeting}! üìö Bienvenido al Archivo Patrimonial.\n\nEscribe tu b√∫squeda o usa **üìÇ Categor√≠as** para explorar."
        ],
        
        'farewell': [
            "¬°Hasta pronto! üëã Vuelve cuando quieras.",
            "¬°Adi√≥s! üåü Fue un gusto ayudarte.",
            "¬°Nos vemos! ÔøΩ Aqu√≠ estar√©."
        ],
        
        'gratitude': [
            "¬°De nada! üòä ¬øAlgo m√°s?",
            "¬°Con gusto! ¬øOtra b√∫squeda?",
            "¬°Me alegra ayudar! üìö"
        ],
        
        'help': [
            """üìö **Archivo Patrimonial UAH**

Busco documentos hist√≥ricos: dictadura, DDHH, fotos, m√∫sica.

**Ejemplos:** "fotos Aylwin", "documentos MIR", "dictadura a√±os 80"

O usa **ÔøΩ Categor√≠as** para explorar. ¬øQu√© buscas?""",
            
            """Soy tu asistente del Archivo Patrimonial. üîç

Tengo: documentos, fotos, m√∫sica (1973-actualidad)

Escribe tu b√∫squeda o prueba **üìÇ Categor√≠as**."""
        ],
        
        'smalltalk': [
            """¬°Buena pregunta! ü§ñ 

Soy un asistente inteligente especializado en el **Archivo Patrimonial de la Universidad Alberto Hurtado**.

**Mi prop√≥sito:**
Ayudar a las personas a descubrir y explorar documentos hist√≥ricos sobre Chile, especialmente:
‚Ä¢ Per√≠odo de dictadura militar (1973-1990)
‚Ä¢ Movimientos sociales y DDHH
‚Ä¢ Memoria colectiva y patrimonio cultural
‚Ä¢ Historia pol√≠tica reciente

**¬øQu√© me hace especial?**
üìö Conozco en detalle los documentos del archivo
üîç Puedo encontrar material espec√≠fico r√°pidamente
üí° Entiendo contexto hist√≥rico y t√©rminos relacionados
üéØ Te ayudo a explorar temas que te interesen

¬øTe gustar√≠a que busque algo sobre la historia de Chile?""",
            
            """üòä Soy el chatbot del Archivo Patrimonial UAH.

Piensa en m√≠ como un **bibliotecario digital especializado** que conoce muy bien el archivo y puede ayudarte a encontrar exactamente lo que buscas sobre la historia de Chile.

**Datos sobre m√≠:**
‚Ä¢ Acceso a miles de documentos hist√≥ricos
‚Ä¢ Especialidad en historia reciente de Chile
‚Ä¢ Conocimiento sobre dictadura, DDHH, movimientos sociales
‚Ä¢ Disponible 24/7 para ayudarte

Cu√©ntame, ¬øqu√© tema te interesa explorar? üìö"""
        ]
    }
    
    # Seleccionar respuesta aleatoria
    if conversation_type in responses:
        return random.choice(responses[conversation_type])
    
    return None

# ============================================================================
# GENERACI√ìN DE RESPUESTAS CON IA
# ============================================================================

def generate_response(query, context_docs, suggestions=None):
    """
    Genera respuesta con IA usando Gemini
    Esta funci√≥n SOLO se llama para b√∫squedas reales (conversation_type='search')
    Ahora incluye sugerencias de refinamiento si se proporcionan
    
    IMPORTANTE: La detecci√≥n de temas administrativos SOLO aplica si NO se
    encontraron documentos. Si hay documentos coincidentes, siempre los muestra.
    """
    suggestions = suggestions or []
    
    # =========================================================================
    # PRIMERO: Si hay documentos encontrados, SIEMPRE mostrarlos
    # Esto evita falsos positivos con palabras como "financiamiento", "programa"
    # que pueden aparecer en t√≠tulos de documentos hist√≥ricos v√°lidos.
    # =========================================================================
    
    if context_docs:
        # HAY DOCUMENTOS - No verificar keywords administrativas
        # Los documentos encontrados son la prioridad
        pass  # Continuar con la generaci√≥n de respuesta normal
    else:
        # NO HAY DOCUMENTOS - Ahora s√≠ verificar si es tema administrativo
        # Solo estas keywords aplican cuando NO se encontr√≥ nada
        administrative_keywords = [
            'matricula', 'matr√≠cula', 'inscripci√≥n', 'inscripcion',
            'horario', 'horarios', 'clases', 'notas', 'calificaciones',
            'malla', 'curricular', 'admisi√≥n', 'admision', 'postular',
            'aranceles', 'becas', 'pago', 'cuota', 'arancel',
            'profesor', 'docente', 'contacto universidad', 'email uah',
            'carrera', 'carreras', 'postgrado', 'magister', 'mag√≠ster',
            'como me inscribo', 'donde queda la universidad', 'telefono uah'
        ]
        
        query_lower = query.lower()
        is_administrative = any(keyword in query_lower for keyword in administrative_keywords)
        
        if is_administrative:
            return """üéì Esta consulta est√° fuera del alcance del Archivo Patrimonial.

üìö El **Archivo Patrimonial UAH** se enfoca en documentos hist√≥ricos y patrimonio cultural chileno (1973-actualidad).

Para informaci√≥n sobre **matr√≠culas, horarios, admisi√≥n y temas acad√©micos**, por favor visita:

üåê **Sitio web oficial**: [www.uahurtado.cl](https://www.uahurtado.cl)
üì± **Instagram UAH**: [@uahurtado](https://www.instagram.com/uahurtado/)
üìß **Email**: [email protected]

---

üí° **¬øSab√≠as que...?** Nuestro archivo contiene documentos fascinantes sobre la historia de Chile. ¬øTe gustar√≠a explorar alg√∫n tema hist√≥rico?"""
    
    # Si no hay documentos relevantes - respuesta ejecutiva corta
    if not context_docs:
        return """No encontr√© resultados para esa b√∫squeda. üîç

**Prueba con:**
‚Ä¢ Aylwin, dictadura, derechos humanos
‚Ä¢ fotograf√≠as, documentos, m√∫sica

O usa **üìÇ Categor√≠as** para explorar el archivo.

¬øQu√© tema buscas?"""
    
    # Si no hay GENAI disponible, respuesta b√°sica
    if not GENAI_AVAILABLE:
        response = "üìö **He encontrado estos documentos relevantes:**\n\n"
        for i, doc in enumerate(context_docs, 1):
            response += f"{i}. **{doc['title']}**\n"
            response += f"   üîó [Ver documento]({doc['href']})\n\n"
        return response
    
    # Construir contexto para la IA
    try:
        context = "Documentos relevantes encontrados:\n\n"
        for i, doc in enumerate(context_docs, 1):
            score = doc.get('relevance_score', 0)
            context += f"{i}. {doc['title']} (relevancia: {score:.2f})\n"
            context += f"   URL: {doc['href']}\n\n"
        
        # Construir secci√≥n de sugerencias si existen
        suggestions_text = ""
        if suggestions:
            suggestions_text = "\n\nSUGERENCIAS DE REFINAMIENTO PARA EL USUARIO:\n"
            for sug in suggestions:
                suggestions_text += f"- {sug['message']}\n"
                if sug.get('options'):
                    for opt in sug['options'][:3]:  # Limitar a 3 opciones
                        suggestions_text += f"  ‚Ä¢ {opt}\n"
        
        prompt = f"""Eres el Asistente Virtual experto del Archivo Patrimonial de la Universidad Alberto Hurtado. Tu misi√≥n principal es ayudar a investigadores y estudiantes que NO son expertos en archiv√≠stica a encontrar documentos hist√≥ricos.

CONTEXTO DEL ARCHIVO:
El archivo se organiza jer√°rquicamente en FONDOS (la colecci√≥n mayor, el origen) y SERIES (las categor√≠as internas).

Los FONDOS principales son:
1. **Presidente Patricio Aylwin (1990-1994)**: Documentos pol√≠ticos, cartas, videos del per√≠odo de transici√≥n democr√°tica.
2. **Iglesias y Dictadura**: Derechos Humanos, Vicar√≠as, revista "No podemos callar" (1973-1990).
3. **M√∫sica Docta Chilena**: Partituras y grabaciones del siglo XX.
4. **Volantes Pol√≠ticos**: Panfletos y propaganda de 1973-1990.
5. **Programa Padres e Hijos (CIDE)**: Fotograf√≠as educativas de Juan Maino (1974-1976).

ANALOG√çAS PARA EXPLICAR (usa cuando el usuario pregunte):
- Fondo = Una serie de TV completa | Serie = Las temporadas o cap√≠tulos
- Fondo = Un √°rbol completo | Serie = Las ramas del √°rbol

DOCUMENTOS ENCONTRADOS:
{context}

CONSULTA DEL USUARIO: "{query}"{suggestions_text}

INSTRUCCIONES DE COMPORTAMIENTO:
1. EDUCA AL USUARIO: Si busca algo gen√©rico (ej: "fotos"), explica brevemente que est√°n organizadas en Fondos espec√≠ficos y sugiere uno.
2. USA ANALOG√çAS: Si preguntan qu√© es un Fondo o Serie, usa las analog√≠as de arriba.
3. POTENCIA TU VALOR: Recu√©rdales que puedes filtrar por contexto (ej: "He encontrado esto en la Serie de Correspondencia del Fondo Aylwin").
4. CONTEXTUALIZA: A√±ade breves notas hist√≥ricas cuando sean relevantes.

FORMATO DE RESPUESTA:
1. **Saludo contextual** breve (ej: "¬°Encontr√© material interesante!")
2. **Lista de documentos** con enlaces: [T√≠tulo](URL)
3. **Contexto del Fondo/Serie** cuando sea relevante
4. **Breve contexto hist√≥rico** si a√±ade valor
5. **Sugerencias** o invitaci√≥n a explorar m√°s

TONO:
- Amable, universitario, claro y pedag√≥gico
- Evita tecnicismos archiv√≠sticos sin explicarlos
- Usa emojis estrat√©gicamente (üìö üìÑ üîç üí° ‚ú® üìÇ)
- Celebra cuando encuentras material relevante
- S√© emp√°tico si no hay resultados

IMPORTANTE: 
- Los enlaces llevan directamente a los documentos
- Si la consulta es muy amplia, sugiere t√©rminos m√°s espec√≠ficos
- Siempre termina invitando a hacer m√°s preguntas

Responde de forma natural, √∫til y educativa:"""

        responder = factory.create_responder()
        resp_text = responder(prompt)
        if resp_text is not None:
            return resp_text

        # Fallback: si no hay GENAI disponible, respuesta b√°sica con sugerencias
        response = "üìö **Encontr√© estos documentos relevantes:**\n\n"
        for i, doc in enumerate(context_docs, 1):
            response += f"{i}. **{doc['title']}**\n"
            response += f"   üîó [Ver documento]({doc['href']})\n\n"
        
        # A√±adir sugerencias si existen
        if suggestions:
            response += "\n---\n\n"
            for sug in suggestions:
                response += f"{sug['message']}\n"
                if sug.get('options'):
                    for opt in sug['options'][:4]:
                        response += f"  ‚Ä¢ {opt}\n"
                response += "\n"
        
        response += "\nüí° **Nota:** Si estos documentos no son exactamente lo que buscabas, intenta refinar tu b√∫squeda con t√©rminos m√°s espec√≠ficos."
        return response
        
    except Exception as e:
        print(f"‚ùå Error con Gemini: {e}")
        traceback.print_exc()

        # Fallback: respuesta sin IA
        response = "üìö **Encontr√© estos documentos relevantes:**\n\n"
        for i, doc in enumerate(context_docs, 1):
            response += f"{i}. **{doc['title']}**\n"
            response += f"   üîó [Ver documento]({doc['href']})\n\n"
        response += "\nüí° **Nota:** Estoy experimentando limitaciones t√©cnicas, pero aqu√≠ est√°n los documentos que coinciden con tu b√∫squeda."
        return response

# ============================================================================
# L√ìGICA DE CONVERSACI√ìN MULTI-TURNO
# ============================================================================

def get_or_create_session(session_id: str) -> ConversationSession:
    """Obtiene o crea una sesi√≥n conversacional para un usuario"""
    if session_id not in conversation_sessions:
        conversation_sessions[session_id] = ConversationSession(session_id)
    return conversation_sessions[session_id]

def handle_follow_up_message(query: str, session: ConversationSession) -> tuple:
    """
    Maneja mensajes de seguimiento (no es la primera b√∫squeda).
    Retorna: (debe_hacer_nueva_busqueda: bool, nueva_query: str, respuesta_ramificacion: str or None)
    
    Usa estrategias inyectadas (DIP): intention_detector, entity_extractor
    """
    # PASO 1: Eliminar saludos del mensaje para detectar intenci√≥n real
    cleaned_query = intention_detector.remove_greetings(query)
    print(f"üßπ Mensaje limpiado de saludos: '{query}' ‚Üí '{cleaned_query}'")
    
    # PASO 2: Detectar si hay b√∫squeda expl√≠cita
    has_search = intention_detector.has_explicit_search(cleaned_query)
    print(f"üîç ¬øHay b√∫squeda expl√≠cita? {has_search}")
    
    # PASO 3: Si NO hay b√∫squeda expl√≠cita, detectar intenci√≥n (satisfecho, insatisfecho, etc)
    if not has_search:
        intention = intention_detector.detect(cleaned_query)
        print(f"üéØ Intenci√≥n detectada: {intention}")
        
        # Caso 1: Usuario satisfecho
        if intention == 'satisfied':
            response = "¬°Excelente! üòä Me alegra haber encontrado lo que buscabas.\n\n¬øHay algo m√°s que quieras explorar en el Archivo Patrimonial?"
            return False, None, response
        
        # Caso 2: Usuario insatisfecho SIN informaci√≥n adicional
        if intention == 'unsatisfied':
            response = """‚ùì Entiendo que no encontraste lo que buscabas. \n\n**Para poder ayudarte mejor, ¬øpodr√≠as ser m√°s espec√≠fico?** ü§î\n\nüí° Por ejemplo:\n‚Ä¢ **Per√≠odo:** ¬øDe qu√© a√±os? (1973-1990, 1980-1985, etc.)\n‚Ä¢ **Tipo:** ¬øFotograf√≠as, testimonios, documentos, reportes?\n‚Ä¢ **Tema:** ¬øHay un aspecto espec√≠fico? (DDHH, partido pol√≠tico, organizaci√≥n)\n‚Ä¢ **Persona:** ¬øHay alguien espec√≠fico involucrado?\n\nCu√©ntame m√°s y har√© una b√∫squeda m√°s dirigida. üìö"""
            return False, None, response
    
    # PASO 4: Si HAY b√∫squeda expl√≠cita, hacer nueva b√∫squeda
    # (ignorar intenci√≥n de satisfacci√≥n/insatisfacci√≥n si hay t√©rminos de b√∫squeda claros)
    entities = entity_extractor.extract(cleaned_query)
    query_parts = []
    
    if entities['topics']:
        query_parts.extend(entities['topics'])
    if entities['years']:
        query_parts.append(f"{min(entities['years'])}")
    if entities['doc_types']:
        query_parts.extend(entities['doc_types'])
    
    # Si no extrajimos nada, usar el query limpiado
    new_query = ' '.join(query_parts) if query_parts else cleaned_query
    print(f"üîç Nueva b√∫squeda ser√°: '{new_query}'")
    
    return True, new_query, None

def compare_and_format_results(new_docs: List[Dict], session: ConversationSession, original_query: str) -> str:
    """
    Compara resultados nuevos con anteriores y formatea respuesta apropiada.
    """
    truly_new, similar = DocumentComparator.find_similar(new_docs, session.get_previous_hrefs())
    
    response = ""
    
    if similar:
        response += "üìå **Documentos encontrados (algunos similares a b√∫squedas anteriores):**\n\n"
        for i, doc in enumerate(new_docs, 1):
            is_similar = "üîÑ " if doc in similar else "‚ú® "
            response += f"{is_similar}{i}. **{doc['title']}**\n"
            response += f"   üîó [Ver documento]({doc['href']})\n\n"
    else:
        response += "üìö **Aqu√≠ est√°n los documentos encontrados con tu b√∫squeda refinada:**\n\n"
        for i, doc in enumerate(new_docs, 1):
            response += f"{i}. **{doc['title']}**\n"
            response += f"   üîó [Ver documento]({doc['href']})\n\n"
    
    # Calcular similitud tem√°tica
    topic_similarity = DocumentComparator.by_topic_similarity(
        session.last_results,
        new_docs
    )
    
    if truly_new:
        response += f"\nüí° Encontr√© **{len(truly_new)} documento(s) nuevo(s)** en esta b√∫squeda que no aparec√≠an antes.\n"
    
    if topic_similarity > 0.6:
        response += f"üîó Estos resultados tienen alta similitud tem√°tica con tu b√∫squeda anterior.\n"
    elif topic_similarity > 0.3:
        response += f"üîÑ Estos resultados comparten algunos temas con la b√∫squeda anterior.\n"
    else:
        response += f"üÜï Estos resultados son bastante diferentes a los anteriores.\n"
    
    response += "\n**¬øEncontraste lo que buscabas?** Si no, cu√©ntame m√°s y seguiremos buscando. üòä"
    
    return response

# ============================================================================
# RUTAS DE LA API
# ============================================================================


@app.route('/api/chat', methods=['POST', 'OPTIONS'])
def chat():
    """
    Endpoint principal del chatbot
    Detecta tipo de conversaci√≥n y responde apropiadamente
    """
    # Manejar preflight CORS
    if request.method == 'OPTIONS':
        response = jsonify({'status': 'ok'})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
        return response, 200
    
    query = ""
    session_id = ""
    try:
        print(f"\n{'='*60}")
        print(f"üì• Nueva solicitud recibida")
        print(f"{'='*60}")
        print(f"üî• M√©todo: {request.method}")
        print(f"üî• Content-Type: {request.content_type}")

        # Obtener query de JSON o form
        data = request.get_json(silent=True)
        if data and isinstance(data, dict):
            query = data.get('query', '')
            session_id = data.get('session_id', 'default')
            print(f"‚úÖ Query from JSON: '{query}'")
        else:
            query = request.form.get('query', '')
            session_id = request.form.get('session_id', 'default')
            print(f"‚úÖ Query from form: '{query}'")

        query = query.strip()
        print(f"üîé Query procesada: '{query}'")
        print(f"üÜî Session ID: {session_id}")
        event_bus.publish('chat.received', {'query': query})

        if not query:
            print("‚ùå Query vac√≠a")
            return jsonify({
                'success': False,
                'error': 'No query provided', 
                'details': 'La consulta no puede estar vac√≠a.'
            }), 400

        # Obtener o crear sesi√≥n del usuario
        session = get_or_create_session(session_id)
        print(f"üìã Session creada/recuperada. Historial: {len(session.search_history)} b√∫squedas")

        # ============ PASO 1: DETECTAR TIPO DE CONVERSACI√ìN PRIMERO ============
        # Esto asegura que saludos, despedidas, etc. se respondan conversacionalmente
        # INCLUSO en mensajes de seguimiento (si es solo "Hola buenas", no debe hacer b√∫squeda)
        conversation_type = detect_conversation_type(query)
        print(f"üéØ Tipo detectado: {conversation_type}")
        event_bus.publish('chat.type_detected', {'type': conversation_type})
        
        # ============ PASO 2: SI ES CONVERSACI√ìN CASUAL, RESPONDER SIN BUSCAR ============
        if conversation_type in ['greeting', 'farewell', 'gratitude', 'help', 'smalltalk']:
            print(f"üí¨ Respuesta conversacional (sin b√∫squeda)")
            response_text = generate_conversational_response(query, conversation_type)
            
            if response_text:
                response_html = markdown.markdown(response_text)
                event_bus.publish('response.generated', {'chars': len(response_text), 'docs': 0})
                
                return jsonify({
                    'success': True,
                    'response': response_html,
                    'documents': [],
                    'embeddings_ready': embeddings_ready,
                    'conversation_type': conversation_type,
                    'session_id': session_id
                })

        # Verificar si es seguimiento ANTES de agregar la b√∫squeda actual
        is_follow_up = session.is_follow_up()
        
        # ============ L√ìGICA DE SEGUIMIENTO ============
        # Si es un mensaje de seguimiento (no la primera b√∫squeda)
        if is_follow_up:
            print(f"üîÑ Mensaje de seguimiento detectado (b√∫squedas previas: {len(session.search_history)})")
            should_search, refined_query, branch_response = handle_follow_up_message(query, session)
            
            if branch_response:
                # Es una ramificaci√≥n de la l√≥gica (satisfecho, pedir detalles, etc)
                print(f"üí¨ Respuesta de ramificaci√≥n: {len(branch_response)} caracteres")
                response_html = markdown.markdown(branch_response)
                event_bus.publish('response.generated', {'chars': len(branch_response), 'docs': 0})
                
                return jsonify({
                    'success': True,
                    'response': response_html,
                    'documents': [],
                    'embeddings_ready': embeddings_ready,
                    'conversation_type': 'follow_up_branch',
                    'session_id': session_id
                })
            
            if not should_search:
                # No hay nueva b√∫squeda que hacer
                return jsonify({
                    'success': True,
                    'response': markdown.markdown("‚úÖ ¬øEn qu√© m√°s te puedo ayudar?"),
                    'documents': [],
                    'embeddings_ready': embeddings_ready,
                    'conversation_type': 'follow_up_satisfied',
                    'session_id': session_id
                })
            
            # Hay nueva b√∫squeda que hacer
            query = refined_query
            print(f"üîç Nueva b√∫squeda con query refinada: '{query}'")

        # ============ FLUJO NORMAL (primera b√∫squeda o b√∫squeda refinada) ============
        # PASO 3: SI ES 'search', BUSCAR DOCUMENTOS (6 documentos) Y SUGERENCIAS
        print(f"üîç Realizando b√∫squeda de documentos...")
        relevant_docs, suggestions = search_documents(query, top_k=6, include_suggestions=True)
        print(f"üìÑ Encontrados {len(relevant_docs)} documentos")
        if suggestions:
            print(f"üí° Generadas {len(suggestions)} sugerencias")
        
        # Registrar b√∫squeda en la sesi√≥n
        session.add_search(query, relevant_docs)

        # PASO 4: GENERAR RESPUESTA CON IA (incluyendo sugerencias)
        print(f"ü§ñ Generando respuesta con IA...")
        response_text = generate_response(query, relevant_docs, suggestions)
        print(f"‚úÖ Respuesta generada: {len(response_text)} caracteres")

        # Convertir markdown a HTML
        response_html = markdown.markdown(response_text)
        event_bus.publish('response.generated', {'chars': len(response_text), 'docs': len(relevant_docs)})

        return jsonify({
            'success': True,
            'response': response_html,
            'documents': relevant_docs,
            'embeddings_ready': embeddings_ready,
            'conversation_type': conversation_type,
            'session_id': session_id
        })

    except Exception as e:
        print(f"\n‚ùå ERROR EN /chat:")
        print(f"{'='*60}")
        traceback.print_exc()
        print(f"{'='*60}\n")
        
        return jsonify({
            'success': False,
            'error': 'Error procesando la consulta',
            'details': str(e)
        }), 500

@app.route('/api/health', methods=['GET'])
def health():
    """Endpoint de health check"""
    return jsonify({
        'status': 'ok',
        'documents_loaded': len(documents),
        'embeddings_loaded': get_embeddings_count(document_embeddings),
        'genai_available': GENAI_AVAILABLE
    })

@app.route('/api/categories', methods=['GET'])
def get_categories():
    """
    Retorna las categor√≠as disponibles para navegaci√≥n
    Incluye: materias (dc:subject), autores (dc:creator), lugares (dc:coverage)
    """
    try:
        # Cargar categor√≠as desde archivo
        categories_file = os.path.join(os.path.dirname(__file__), 'categories.json')
        
        if os.path.exists(categories_file):
            with open(categories_file, 'r', encoding='utf-8') as f:
                all_categories = json.load(f)
            
            # Limitar a top 100 por categor√≠a para UI
            result = {
                'materias': all_categories.get('materias', [])[:100],
                'autores': all_categories.get('autores', [])[:100],
                'lugares': all_categories.get('lugares', [])[:100]
            }
            
            return jsonify({
                'success': True,
                'categories': result,
                'total': {
                    'materias': len(all_categories.get('materias', [])),
                    'autores': len(all_categories.get('autores', [])),
                    'lugares': len(all_categories.get('lugares', []))
                }
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Archivo de categor√≠as no encontrado'
            }), 404
            
    except Exception as e:
        print(f"Error cargando categor√≠as: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/search-by-category', methods=['POST'])
def search_by_category():
    """
    Busca documentos filtrando por categor√≠a espec√≠fica
    Body: { category_type: 'materias'|'autores'|'lugares', category_name: 'Derechos Humanos' }
    """
    try:
        data = request.get_json()
        category_type = data.get('category_type', '')
        category_name = data.get('category_name', '')
        
        if not category_type or not category_name:
            return jsonify({
                'success': False,
                'error': 'Se requiere category_type y category_name'
            }), 400
        
        # Mapeo de tipos a campos del JSON
        field_map = {
            'materias': 'dc:subject',
            'autores': 'dc:creator',
            'lugares': 'dc:coverage'
        }
        
        field = field_map.get(category_type)
        if not field:
            return jsonify({
                'success': False,
                'error': 'Tipo de categor√≠a inv√°lido'
            }), 400
        
        # Cargar documentos con metadatos
        metadata_file = os.path.join(os.path.dirname(__file__), 'clean_with_metadata.json')
        
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8', errors='ignore') as f:
                docs_with_metadata = json.load(f)
        else:
            # Fallback a documentos normales
            docs_with_metadata = documents
        
        # Filtrar documentos que contengan la categor√≠a
        results = []
        category_lower = category_name.lower()
        
        for doc in docs_with_metadata:
            field_values = doc.get(field, [])
            if isinstance(field_values, list):
                for val in field_values:
                    if category_lower in val.lower():
                        results.append({
                            'title': doc.get('title', doc.get('dc:title', 'Sin t√≠tulo')),
                            'href': doc.get('href', ''),
                            'subject': doc.get('dc:subject', [])[:3],
                            'creator': doc.get('dc:creator', [])[:2],
                            'coverage': doc.get('dc:coverage', [])
                        })
                        break
            
            if len(results) >= 15:
                break
        
        return jsonify({
            'success': True,
            'category_type': category_type,
            'category_name': category_name,
            'results': results,
            'count': len(results)
        })
        
    except Exception as e:
        print(f"Error en b√∫squeda por categor√≠a: {e}")
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/', methods=['GET'])
def index():
    """Ruta ra√≠z que carga el Frontend unificado"""
    return render_template('index.html')


# ============================================================================
# MAIN: EJECUTAR APLICACI√ìN
# ============================================================================

if __name__ == '__main__':
    print("\n" + "="*70)
    print("üöÄ INICIANDO CHATBOT DEL ARCHIVO PATRIMONIAL UAH")
    print("="*70)
    print(f"üìä Documentos cargados: {len(documents)}")
    print(f"üß† Embeddings disponibles: {get_embeddings_count(document_embeddings)}")
    print(f"ü§ñ Gemini API: {'‚úÖ Disponible' if GENAI_AVAILABLE else '‚ùå No disponible'}")
    print(f"üåê Servidor Flask: http://localhost:5000")
    print(f"üîó Frontend esperado: http://localhost:8080 (v√≠a Nginx)")
    print(f"üì° Endpoint principal: POST /api/chat")
    print(f"‚ù§Ô∏è Health check: GET /api/health")
    print("="*70)
    print("‚úÖ Sistema listo para recibir consultas!\n")

    app.run(
        debug=False,
        host='0.0.0.0',
        port=5000,
        threaded=True
    )